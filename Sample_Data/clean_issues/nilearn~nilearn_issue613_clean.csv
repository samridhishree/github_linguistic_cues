rectype,issueid,project_owner,project_name,actor,time,text,action,title,clean_text
rectype,issueid,project_owner,project_name,actor,time,text,action,title,text
issue_title,613,nilearn,nilearn,lesteve,2015-06-17 13:42:23,"The idea is to reduce memory usage to be able to tackle data like HCP.
",start issue,[WIP] IncrementalPCA in CanICA,the idea reduc memori usag abl tackl data like hcp
pull_request_title,613,nilearn,nilearn,lesteve,2015-06-17 13:42:23,"The idea is to reduce memory usage to be able to tackle data like HCP.
",ae5c17940725240e900f689445bc6de4d02d7d34,[WIP] IncrementalPCA in CanICA,the idea reduc memori usag abl tackl data like hcp
issue_comment,613,nilearn,nilearn,lesteve,2015-06-17 13:50:13,"IncrementalPCA only exists in scikit-learn 0.16.1. Because of that the only tests we expect to pass at the moment is the python 3.4 ones using the latest versions of scikit-learn.

To do/questions:
- [ ] Failing tests: for some reason giving twice the same data doesn't produce the same eigenvectors in MultiPCA. Not sure how bad this is. **Edit**: very likely related to https://github.com/nilearn/nilearn/pull/829.
- [x] Do we want to make this feature available for scikit-learn versions older than 0.16 ? From a quick look at it, it would be slightly painful.
- [x] How do we want to support this feature, a boolean parameter in CanICA to toggle IncrementalPCA (False by default) ?
- [x] Caching the result of the svd to mirror the previous behaviour
",,,incrementalpca exist scikitlearn 0161 becaus test expect pass moment python 34 one use latest version scikitlearn To doquest fail test reason give twice data doesnt produc eigenvector multipca not sure bad edit like relat x Do want make featur avail scikitlearn version older 016 from quick look would slightli pain x how want support featur boolean paramet canica toggl incrementalpca fals default x cach result svd mirror previou behaviour
issue_comment,613,nilearn,nilearn,lesteve,2015-06-17 14:01:00,"Also not sure how to evaluate the quality of the maps.

The following plots were obtained from the example script: examples/connectivity/plot_canica_resting_state.py.

### master

![canica_plots_master](https://cloud.githubusercontent.com/assets/1680079/8208820/e98eb720-1509-11e5-8913-7061a608f97b.png)

### this PR

![canica_plots_mine](https://cloud.githubusercontent.com/assets/1680079/8208836/fcaef874-1509-11e5-9c87-90c32a1af9ce.png)
",,,also sure evalu qualiti map the follow plot obtain exampl script examplesconnectivityplotcanicarestingstatepi master canicaplotsmast PR canicaplotsmin
issue_comment,613,nilearn,nilearn,lesteve,2015-06-17 14:26:04,"Some memory_profiler plots. Data is from @mrahim (221 subjects from ADNI, ~18GB, 40 components):

### master

![canica_master_memory_profile](https://cloud.githubusercontent.com/assets/1680079/8209293/89a79edc-150c-11e5-9c0d-b9e96585ed91.png)

### this PR

![canica_incremental_pca_memory_profile](https://cloud.githubusercontent.com/assets/1680079/8209306/93eda1f2-150c-11e5-9a87-d5b98356eec9.png)

Memory usage decreased from 8GB to 2.5GB.
Running time increased by a factor 2.
",,,some memoryprofil plot data mrahim 221 subject adni 18gb 40 compon master canicamastermemoryprofil PR canicaincrementalpcamemoryprofil memori usag decreas 8gb 25gb run time increas factor 2
issue_comment,613,nilearn,nilearn,bthirion,2015-06-17 15:33:44,"On 17/06/2015 16:01, Loïc Estève wrote:

> Also not sure how to evaluate the quality of the maps.
> 
> It is hard to say anything on quality, but it is useful to quantify how 
> different the result is between two implementations. For this, I would 
> use a hungarian algorithm and report the sums of correlations of matched 
> components.
",,,On 17062015 1601 loïc estèv wrote
issue_comment,613,nilearn,nilearn,GaelVaroquaux,2015-06-17 21:49:31,"Great job! The maps look good. No problem with the variability that you observe.

The memory reduction is great! It would be nice to use a profiler to see where the cost comes in terms of time. I suspect that doing a PR to scikit-learn to use randomized_svd in the IncrementalPCA would give you the speed back.

Finally: we won't try to support this for versions of scikit-learn older than 0.16.
",,,great job the map look good No problem variabl observ the memori reduct great It would nice use profil see cost come term time I suspect PR scikitlearn use randomizedsvd incrementalpca would give speed back final wont tri support version scikitlearn older 016
issue_comment,613,nilearn,nilearn,lesteve,2015-06-19 06:17:02,"I changed a few things, most notably the fact that there is a `group_incremental_pca` parameter in CanICA to use IncrementalPCA for the group PCA.

As @ogrisel noticed the IncrementalPCA is going to center the data in contrary to randomized_svd, which is what is used in master, so we shouldn't expect matching between the two.

Still haven't figured out why the tests were failing.
",,,I chang thing notabl fact paramet canica use incrementalpca group pca As ogrisel notic incrementalpca go center data contrari randomizedsvd use master shouldnt expect match two still havent figur test fail
issue_comment,613,nilearn,nilearn,banilo,2015-06-27 07:40:10,"> Failing tests: for some reason giving twice the same data doesn't produce the same eigenvectors in MultiPCA. Not sure how bad this is.

Given SVD should return the same result every time, is it not more likely that non-identical eigenvectors from MultiPCA are related to a) multi-processing and/or b) caching mechansims. Perhaps debug by ruling out joblib sources...?
",,,given svd return result everi time like nonident eigenvector multipca relat multiprocess andor b cach mechansim perhap debug rule joblib sourc
issue_comment,613,nilearn,nilearn,lesteve,2015-06-29 06:58:53,"> Given SVD should return the same result every time, is it not more likely that non-identical eigenvectors from MultiPCA are related to a) multi-processing and/or b) caching mechansims. Perhaps debug by ruling out joblib sources...?

I am afraid there is a misunderstanding here. The failing test is [here](https://github.com/nilearn/nilearn/blob/master/nilearn/decomposition/tests/test_multi_pca.py#L35):

``` python
components1 = multi_pca.fit(data).components_
components2 = multi_pca.fit(2 * data).components_
np.testing.assert_array_almost_equal(components1, components2)
```

`data` is a list of Nifti images. Twice the same data means `2 * data`. Unfortunately I don't have a good enough understanding of CanICA to know how important this test failure is.
",,,I afraid misunderstand the fail test list nifti imag twice data mean unfortun I dont good enough understand canica know import test failur
issue_comment,613,nilearn,nilearn,arthurmensch,2015-07-03 10:51:59,"MultiPCA output different results depending on the run because of a lack of `random_state` in `session_pca`, which critically call `randomized_svd` when data is too large. I have a working branch where this issue is solved, I should clean it before proposing it.

See PR #630 
",,,multipca output differ result depend run lack critic call data larg I work branch issu solv I clean propos see PR 630
issue_comment,613,nilearn,nilearn,bthirion,2015-07-03 11:08:37,"On 03/07/2015 12:51, Arthur Mensch wrote:

> MultiPCA output different results depending on the run because of a 
> lack of |random_state| in |session_pca|. I have a working branch where 
> this issue is solved, I should clean it before proposing it.
> 
> —
> Reply to this email directly or view it on GitHub 
> https://github.com/nilearn/nilearn/pull/613#issuecomment-118314510.
> 
> Would be great indeed.
",,,On 03072015 1251 arthur mensch wrote
issue_comment,613,nilearn,nilearn,AlexandreAbraham,2015-07-03 13:20:00,"> MultiPCA output different results depending on the run because of a lack of random_state in session_pca, which critically call randomized_svd when data is too large. I have a working branch where this issue is solved, I should clean it before proposing it.

I think that the problem is that the results are stable when using `randomized_pca` and not stable when using incremental PCA.
",,,I think problem result stabl use stabl use increment pca
issue_comment,613,nilearn,nilearn,lesteve,2016-04-26 07:56:40,"I rescucitated this PR after the changes that went in related to the DictionaryLearning. Here are the updated maps coming from `examples/03_connectivity/plot_canica_resting_state.py`:

#### Master

![canica_components_master](https://cloud.githubusercontent.com/assets/1680079/14810739/b699d3ea-0b94-11e6-8714-155e8ec05f17.png)

#### This PR with incremental_group_pca=True

![canica_components_pr_incremental_group_pca](https://cloud.githubusercontent.com/assets/1680079/14810743/bca01ace-0b94-11e6-8240-edefc0b24475.png)

There is still the same test failing as previously, not sure what to do about it.
",,,I rescucit PR chang went relat dictionarylearn here updat map come master canicacomponentsmast thi PR incrementalgrouppcatru canicacomponentsprincrementalgrouppca there still test fail previous sure
issue_comment,613,nilearn,nilearn,lesteve,2016-05-23 08:59:30,"There is also this comment: https://github.com/nilearn/nilearn/pull/613#discussion_r61042980. Not sure whether we care how we split the data into batches to call partial_fit on.
",,,there also comment not sure whether care split data batch call partialfit
issue_comment,613,nilearn,nilearn,bthirion,2016-05-22 14:57:28,"I don't see any blocker for merging that one.
",,,I dont see blocker merg one
issue_comment,613,nilearn,nilearn,AlexandreAbraham,2016-05-23 08:11:06,"> I don't see any blocker for merging that one.

This one is still not merged because of weird instabilities when using Incremental PCA compared to regular or randomized PCA. If you say this is OK, we can lower the requirements to make the test pass and ignore these results.
",,,thi one still merg weird instabl use increment pca compar regular random pca If say OK lower requir make test pass ignor result
pull_request_commit_comment,613,nilearn,nilearn,banilo,2015-06-24 14:07:28,"How about a simple warning instead of an exception in IncrementalPCA should not be available?
",ae5c17940725240e900f689445bc6de4d02d7d34,"(None, '', u'nilearn/decomposition/multi_pca.py')",how simpl warn instead except incrementalpca avail
pull_request_commit_comment,613,nilearn,nilearn,arthurmensch,2015-07-03 10:57:12,"Is this the canonical way of hiding cached functions ? This is not what is used in other parts of nilearn. It looks clean though
",ae5c17940725240e900f689445bc6de4d02d7d34,"(None, '', u'nilearn/decomposition/multi_pca.py')",Is canon way hide cach function thi use part nilearn It look clean though
pull_request_commit_comment,613,nilearn,nilearn,arthurmensch,2015-07-03 11:18:57,"I think we should work out a way to lazy load subject pcas : On HCP we typically use 100 components PCA on 1650 records, which approximately reduce the whole data set (1,5TB) of a factor 10 (1200 samples per records originaly) : `subject_pcas` will be around 150GB, which is too much for a laptop.

The idea would be to mini-batch subjects pcas so as to benefit from parallelization : we should partial fit on batches of `subject_pcas` of size around `n_jobs`. This would require to put Parallel call into `session_pca`, and transform this function into a generator.

Theoretically, we should then keep in memory `batch_size * single_subject_pca_size * 2`, which should be under 2GB even on HCP.
",ae5c17940725240e900f689445bc6de4d02d7d34,"(None, '', u'nilearn/decomposition/multi_pca.py')",I think work way lazi load subject pca On hcp typic use 100 compon pca 1650 record approxim reduc whole data set 15tb factor 10 1200 sampl per record originali around 150gb much laptop the idea would minibatch subject pca benefit parallel partial fit batch size around thi would requir put parallel call transform function gener theoret keep memori 2gb even hcp
pull_request_commit_comment,613,nilearn,nilearn,lesteve,2015-07-03 12:42:51,"Another approach which is probably easier to implement (proposed by @GaelVaroquaux): use joblib.cache_and_shelve to reduce the memory usage.
",ae5c17940725240e900f689445bc6de4d02d7d34,"(None, '', u'nilearn/decomposition/multi_pca.py')",anoth approach probabl easier implement propos gaelvaroquaux use joblibcacheandshelv reduc memori usag
pull_request_commit_comment,613,nilearn,nilearn,AlexandreAbraham,2015-07-03 12:56:11,"> This would require to put Parallel call into session_pca, and transform this function into a generator.

This code already exists and is part of a PR that will be proposed after #585 is merged. I may be able to extract this particular part if you want.

> use joblib.cache_and_shelve to reduce the memory usage.

Not a good idea because of the IO overhead.
",ae5c17940725240e900f689445bc6de4d02d7d34,"(None, '', u'nilearn/decomposition/multi_pca.py')",thi code alreadi exist part PR propos 585 merg I may abl extract particular part want not good idea IO overhead
pull_request_commit_comment,613,nilearn,nilearn,AlexandreAbraham,2015-07-03 13:01:32,"This is the proper way if you are in function. Here, this code could be optimized by inheriting CacheMixin and using self._cache.
",ae5c17940725240e900f689445bc6de4d02d7d34,"(None, '', u'nilearn/decomposition/multi_pca.py')",thi proper way function here code could optim inherit cachemixin use selfcach
pull_request_commit_comment,613,nilearn,nilearn,GaelVaroquaux,2015-07-03 13:25:25,"> I think we should work out a way on lazy loading subject pcas : On HCP we
> typically use 100 components for 1650 records, which approximately reduce the
> whole data set of a factor 10 (1200 samples per records) : subject_pcas will be
> around 150GB, which is too much for a laptop.

The right thing to do is probably to use joblib shelving:
https://pythonhosted.org/joblib/memory.html#shelving-using-references-to-cached-values

Talk to @AlexandreAbraham: he has been using this in his related code.
",ae5c17940725240e900f689445bc6de4d02d7d34,"(None, '', u'nilearn/decomposition/multi_pca.py')",the right thing probabl use joblib shelv talk alexandreabraham use relat code
pull_request_commit_comment,613,nilearn,nilearn,GaelVaroquaux,2015-07-03 13:26:45,"> ```
> use joblib.cache_and_shelve to reduce the memory usage.
> ```
> 
> Not a good idea because of the IO overhead.

Should be optional, with an auto parameter that turns it on if the list
of subjects is very big (eg more than 50).
",ae5c17940725240e900f689445bc6de4d02d7d34,"(None, '', u'nilearn/decomposition/multi_pca.py')",should option auto paramet turn list subject big eg 50
pull_request_commit_comment,613,nilearn,nilearn,lesteve,2016-04-26 08:01:53,"This is a bit hacky because now subject_pcas come in a single array instead of a list of per-subject PCA and the concatenation happens a bit deep in the BaseDecomposition code. I did that with the old code in mind but maybe I don't actually need to bother doing it ...
",ae5c17940725240e900f689445bc6de4d02d7d34,"(99, '', u'nilearn/decomposition/multi_pca.py')",thi bit hacki subjectpca come singl array instead list persubject pca concaten happen bit deep basedecomposit code I old code mind mayb I dont actual need bother
pull_request_commit,613,nilearn,nilearn,lesteve,2016-04-25 14:37:52,wip,ae5c17940725240e900f689445bc6de4d02d7d34,,wip
