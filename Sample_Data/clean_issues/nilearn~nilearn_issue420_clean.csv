rectype,issueid,project_owner,project_name,actor,time,text,action,title,clean_text
rectype,issueid,project_owner,project_name,actor,time,text,action,title,text
issue_title,420,nilearn,nilearn,AlexandreAbraham,2015-02-10 10:22:16,"For several people, the caching directory is seen as a big pile of junk that grows without able to clean specific masked datasets (as they are all identified by a hash). We should find a solution to make it clearer for the users.
",start issue,Caching system is cryptic for some people,for sever peopl cach directori seen big pile junk grow without abl clean specif mask dataset identifi hash We find solut make clearer user
issue_closed,420,nilearn,nilearn,AlexandreAbraham,2015-04-08 08:58:52,,closed issue,Caching system is cryptic for some people,
issue_comment,420,nilearn,nilearn,GaelVaroquaux,2015-02-10 13:13:28,"> For several people, the caching directory is seen as a big pile of junk that
> grows without able to clean specific masked datasets (as they are all
> identified by a hash). We should find a solution to make it clearer for the
> users.

This has recently come up in joblib. I suggest prepending a very short
description of the inputs to the hash in the directory name. Make
the description of the inputs both short and meaningful will not be
trivial.
",,,thi recent come joblib I suggest prepend short descript input hash directori name make descript input short meaning trivial
issue_comment,420,nilearn,nilearn,GaelVaroquaux,2015-02-10 13:28:18,"> I think that it's mainly useful for dataset masking. However, we don't
> know the which dataset is used at masking time. So I think that we
> should let the user decide of this. Depending on the usage, I'm sure
> than any user can come up with a unique ID.

OK. Part of the problem will be addressed by a functionning cache
replacement policy system in joblib.

The other part of the problem is a general provenance/information problem
that is so far an open problem in software (nobody has solved it in a
good way).

That said, the relevant information is in the joblib store: in each
result directory there is a 'metadata.json'

A function to crawl such information and give a good view of the store
would probably be useful. For instance a 'list' method on the 'memory'
object, that could take an optional function as an argument (in which
case it would list only the content of the cache for this function.

I think that this would be useful. Can you open an issue on the joblib
tracker?
",,,OK part problem address function cach replac polici system joblib the part problem gener provenanceinform problem far open problem softwar nobodi solv good way that said relev inform joblib store result directori metadatajson A function crawl inform give good view store would probabl use for instanc list method memori object could take option function argument case would list content cach function I think would use can open issu joblib tracker
issue_comment,420,nilearn,nilearn,AlexandreAbraham,2015-02-10 13:20:00,"I think that it's mainly useful for dataset masking. However, we don't know the which dataset is used at masking time. So I think that we should let the user decide of this. Depending on the usage, I'm sure than any user can come up with a unique ID.
",,,I think mainli use dataset mask howev dont know dataset use mask time So I think let user decid depend usag Im sure user come uniqu ID
issue_comment,420,nilearn,nilearn,AlexandreAbraham,2015-02-10 13:41:05,"I don't think that people will bother crawling a lot of folders using such a function to clean their directory.
",,,I dont think peopl bother crawl lot folder use function clean directori
issue_comment,420,nilearn,nilearn,GaelVaroquaux,2015-02-10 14:23:45,"Then it's a problem we cannot solve.

Sent from my phone. Please forgive brevity and mis spelling

On Feb 10, 2015, 14:41, at 14:41, Alexandre Abraham notifications@github.com wrote:

> I don't think that people will bother crawling a lot of folders using
> such a function to clean their directory.
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/nilearn/nilearn/issues/420#issuecomment-73700197
",,,then problem cannot solv sent phone pleas forgiv breviti mi spell On feb 10 2015 1441 1441 alexandr abraham notificationsgithubcom wrote
issue_comment,420,nilearn,nilearn,AlexandreAbraham,2015-02-10 15:57:18,"I talked with Salma during the coffee break.
1. In all examples, the maskers have `""nilearn_cache""` as cache directory. We should replace that by `""nilearn_cache/dataset_name""`. Because when users realize this fact, it's already too late and they have a huge cache directory.
2. The other problem is when you mask a dataset of 200 subjects, and then change a parameter (typically smoothing). Even if the first problem is solved, you will end up will 400 folders with hashed names in your cache. Having a function that reads _metadata.json_ files is not helping because a script is still needed to parse the result and delete only a subset of the files. For this problem, we would need a 2-level cache like this:

```
nilearn_cache_adhd
├ hash('filter_and_mask(all_params_but_filepath_smoothing_6)')
│  ├ hash('filepath_1')
│  ├ hash('filepath_2')
│  └ ...
└ hash('filter_and_mask(all_params_but_filepath_smoothing_8)')
   ├ hash('filepath_1')
   ├ hash('filepath_2')
   └ ...
```

This is a quick suggestion, I haven't really thought about this.
",,,I talk salma coffe break 1 In exampl masker cach directori We replac becaus user realiz fact alreadi late huge cach directori 2 the problem mask dataset 200 subject chang paramet typic smooth even first problem solv end 400 folder hash name cach have function read metadatajson file help script still need pars result delet subset file for problem would need 2level cach like thi quick suggest I havent realli thought
issue_comment,420,nilearn,nilearn,GaelVaroquaux,2015-02-10 16:21:21,"> 1. In all examples, the maskers have ""nilearn_cache"" as cache directory. We
>    should replace that by ""nilearn_cache/dataset_name"". Because when users
>    realize this fact, it's already too late and they have a huge cache
>    directory.

Yes and no: there are common things across the different datasets, and
the benefit of putting everything in the same directory is that these
common things are not duplicated or recomputed.

> 1. The other problem is when you mask a dataset of 200 subjects, and then
>    change a parameter (typically smoothing). Even if the first problem is
>    solved, you will end up will 400 folders with hashed names in your cache.
>    Having a function that reads metadata.json files is not helping because a
>    script is still needed to parse the result and delete only a subset of the
>    files. For this problem, we would need a 2-level cache like this:
> 
> nilearn_cache_adhd
> ├ hash('filter_and_mask(all_params_but_filepath_smoothing_6)')
> │  ├ hash('filepath_1')
> │  ├ hash('filepath_2')
> │  └ ...

That's always going to be custom and never going to scale, because you
need to know which parameters should go where.

From what I hear, all these problems are problems that we cannot solve.

What we can do, is try to implement a cache replacement policy, and be
able to limit the disk occupied by caching. This is on my radar.
",,,ye common thing across differ dataset benefit put everyth directori common thing duplic recomput that alway go custom never go scale need know paramet go from I hear problem problem cannot solv what tri implement cach replac polici abl limit disk occupi cach thi radar
issue_comment,420,nilearn,nilearn,AlexandreAbraham,2015-02-10 16:41:30,"> Yes and no: there are common things across the different datasets, and the benefit of putting everything in the same directory is that these common things are not duplicated or recomputed.

Do you have something in mind? Because I can't think of one.

> That's always going to be custom and never going to scale, because you need to know which parameters should go where.

That works for the masker: the first level is everything but the filepath.

> What we can do, is try to implement a cache replacement policy, and be able to limit the disk occupied by caching. This is on my radar.

My guess is that people will get scared if things that used to run smoothly (because cached) become slow (because cache has been invalidated).
",,,Do someth mind becaus I cant think one that work masker first level everyth filepath My guess peopl get scare thing use run smoothli cach becom slow cach invalid
issue_comment,420,nilearn,nilearn,GaelVaroquaux,2015-02-10 16:45:10,"> Do you have something in mind? Because I can't think of one.

Yes. Loading the haxby dataset and applying to it 2 different
classifiers.

> ```
> That's always going to be custom and never going to scale, because
> you need to know which parameters should go where.
> ```
> 
> That works for the masker: the first level is everything but the filepath.

Yes, but it means that you need to hand craft this everywhere, which is
really what I am trying to avoid.

> ```
> What we can do, is try to implement a cache replacement policy, and
> be able to limit the disk occupied by caching. This is on my radar.
> ```
> 
> My guess is that people will get scared if things that used to run smoothly
> (because cached) become slow (because cache has been invalidated).

People have long stopped understanding how a computer works. There are
caching mechanisms everywhere in a computer. Some day things are fast.
Other days they are slow. The world is still a better place with caching
:).
",,,ye load haxbi dataset appli 2 differ classifi ye mean need hand craft everywher realli I tri avoid peopl long stop understand comput work there cach mechan everywher comput some day thing fast other day slow the world still better place cach
issue_comment,420,nilearn,nilearn,AlexandreAbraham,2015-02-10 19:27:19,"> Yes. Loading the haxby dataset and applying to it 2 different classifiers.

I see no problem in changing `nilearn_cache` to `nilearn_cache/haxby` for that particular task.

> Yes, but it means that you need to hand craft this everywhere, which is really what I am trying to avoid.

I was just thinking of the masker for that point.

> People have long stopped understanding how a computer works. There are caching mechanisms everywhere in a computer. Some day things are fast. Other days they are slow. The world is still a better place with caching :).

I have no strong feeling about that. I just had several complaints about a `nilearn_cache` directory growing out of control and people not wanting to delete it by fear of losing all their cache.
",,,I see problem chang particular task I think masker point I strong feel I sever complaint directori grow control peopl want delet fear lose cach
issue_comment,420,nilearn,nilearn,GaelVaroquaux,2015-02-12 07:17:08,"> I see no problem in changing nilearn_cache to nilearn_cache/haxby for that
> particular task.

OK. Point taken. I agree with you. Sorry, I was being dumb. I would
welcome a joblib cache per dataset as long as we don't have a cache
replacement policy.

> I was just thinking of the masker for that point.

I am trying to minimize the amount of code that goes 

> I have no strong feeling about that. I just had several complaints
> about a nilearn_cache directory growing out of control and people not
> wanting to delete it by fear of losing all their cache.

That tells me we need cache replacement policy :). That's difficult work,
but it is feasible, and it will solve many problems at once.
",,,OK point taken I agre sorri I dumb I would welcom joblib cach per dataset long dont cach replac polici I tri minim amount code goe that tell need cach replac polici that difficult work feasibl solv mani problem
issue_comment,420,nilearn,nilearn,AlexandreAbraham,2015-02-12 08:47:50,"> OK. Point taken. I agree with you. Sorry, I was being dumb. I would welcome a joblib cache per dataset as long as we don't have a cache replacement policy.

Cool, I'll do a PR to fix that.

> I am trying to minimize the amount of code that goes

I agree. Adding a subdirectory to nilearn_cache is basically the idea of a two level cache but handled at user level ;). I think it can be more useful to sensibilize users to this problem rather than doing everything for them.

> That tells me we need cache replacement policy :). That's difficult work, but it is feasible, and it will solve many problems at once.

I'm not sure that it will solve the general problem, but I guess that it's better than nothing ;).
",,,cool ill PR fix I agre ad subdirectori nilearncach basic idea two level cach handl user level I think use sensibil user problem rather everyth Im sure solv gener problem I guess better noth
issue_comment,420,nilearn,nilearn,banilo,2015-02-12 08:49:52,"Loosly related, how about an explicit caching-related example to make the various flavors and advantages clear in a neuroimaging context?
",,,loosli relat explicit cachingrel exampl make variou flavor advantag clear neuroimag context
issue_comment,420,nilearn,nilearn,GaelVaroquaux,2015-02-12 08:54:30,"I'd rather rework completely the docs, before adding advanced
documentation and examples. Do you want to take some time to brainstorm
on reworking the docs? I think that it would be very useful.
",,,Id rather rework complet doc ad advanc document exampl Do want take time brainstorm rework doc I think would use
issue_comment,420,nilearn,nilearn,AlexandreAbraham,2015-02-12 09:23:15,":+1:
",,,1
