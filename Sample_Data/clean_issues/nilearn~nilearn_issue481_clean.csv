rectype,issueid,project_owner,project_name,actor,time,text,action,title,clean_text
rectype,issueid,project_owner,project_name,actor,time,text,action,title,text
issue_title,481,nilearn,nilearn,GaelVaroquaux,2015-03-04 00:50:08,"Experience from the Leipzig teaching: we need smaller / faster download.

Specifically, 
1. we should have a zip file for fetch_adhd(n_subjects=1) to download only one subject.
2. fetch_habxy(n_subjects=1) should download as little as posible: no stimuli and only 1 subject.
",start issue,Faster / smaller datasets to download,experi leipzig teach need smaller faster download specif 1 zip file fetchadhdnsubjects1 download one subject 2 fetchhabxynsubjects1 download littl posibl stimuli 1 subject
issue_closed,481,nilearn,nilearn,AlexandreAbraham,2016-06-07 22:01:30,,closed issue,Faster / smaller datasets to download,
issue_comment,481,nilearn,nilearn,AlexandreAbraham,2015-03-04 01:14:03,"> we should have a zip file for fetch_adhd(n_subjects=1) to download only one subject.

What is doable:
- split every subject in a single file
- create another fetcher for ADHD versions preprocessed by the neurobureau. They have 200 subjects and provide directly timeseries for given atlases

> fetch_habxy(n_subjects=1) should download as little as posible: no stimuli and only 1 subject.

This is what `fetch_haxby` does by default. Some masks are also downloaded by default but they are very small.
",,,what doabl split everi subject singl file creat anoth fetcher adhd version preprocess neurobureau they 200 subject provid directli timeseri given atlas thi default some mask also download default small
issue_comment,481,nilearn,nilearn,bcipolli,2015-03-04 01:36:48,"Two of the examples use only one subject from the haxby dataset, but don't pass the `n_subjects=1` argument.  Seems this would be an issue as well.
",,,two exampl use one subject haxbi dataset dont pass argument seem would issu well
issue_comment,481,nilearn,nilearn,GaelVaroquaux,2015-03-04 06:58:50,"> Two of the examples use only one subject from the haxby dataset, but
> don't pass the n_subjects=1 argument. Seems this would be an issue as
> well.

I think that n_subjects=1 should be the default, based on my experience
teaching yesterday.
",,,I think nsubjects1 default base experi teach yesterday
issue_comment,481,nilearn,nilearn,AlexandreAbraham,2015-04-15 12:32:46,"Shouldn't we postpone this one? We'll have no time to solve it before the release.
",,,shouldnt postpon one well time solv releas
issue_comment,481,nilearn,nilearn,GaelVaroquaux,2015-04-15 14:51:21,"> Shouldn't we postpone this one?

Yes, :-$
",,,ye
issue_comment,481,nilearn,nilearn,AlexandreAbraham,2015-04-15 15:16:31,"FYI, dividing ADHD in smaller chunks should not be difficult, I can at least do this one if you want.
",,,fyi divid adhd smaller chunk difficult I least one want
issue_comment,481,nilearn,nilearn,GaelVaroquaux,2015-06-09 18:12:28,"This issue is important. It is screwing up our workshop again. 
",,,thi issu import It screw workshop
issue_comment,481,nilearn,nilearn,AlexandreAbraham,2015-06-09 19:33:40,"can you be more specific ? which dataset is problematic ?
",,,specif dataset problemat
issue_comment,481,nilearn,nilearn,GaelVaroquaux,2015-06-09 19:39:49,"Haxby. We need to break it up in subjects and download only 1 subjects by default. 

Sent from my phone. Please forgive brevity and mis spelling

On Jun 9, 2015, 12:33, at 12:33, Alexandre Abraham notifications@github.com wrote:

> can you be more specific ? which dataset is problematic ?
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/nilearn/nilearn/issues/481#issuecomment-110478512
",,,haxbi We need break subject download 1 subject default sent phone pleas forgiv breviti mi spell On jun 9 2015 1233 1233 alexandr abraham notificationsgithubcom wrote
issue_comment,481,nilearn,nilearn,AlexandreAbraham,2015-06-09 19:41:36,"This is already the case.
",,,thi alreadi case
issue_comment,481,nilearn,nilearn,AlexandreAbraham,2015-06-09 19:45:21,"If you need one dataset to be broken into pieces or moved to nitrc, I can hack it right now. But I have to be sure to target the right ones ;).
",,,If need one dataset broken piec move nitrc I hack right but I sure target right one
issue_comment,481,nilearn,nilearn,GaelVaroquaux,2015-06-09 20:51:47,"Don't hack anything. It not that urgent. For it to be beneficial we need to release. It won't happen today. 

Sent from my phone. Please forgive brevity and mis spelling

On Jun 9, 2015, 12:45, at 12:45, Alexandre Abraham notifications@github.com wrote:

> If you need one dataset to be broken into pieces or moved to nitrc, I
> can hack it right now. But I have to be sure to target the right ones
> ;).
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/nilearn/nilearn/issues/481#issuecomment-110482987
",,,dont hack anyth It urgent for benefici need releas It wont happen today sent phone pleas forgiv breviti mi spell On jun 9 2015 1245 1245 alexandr abraham notificationsgithubcom wrote
issue_comment,481,nilearn,nilearn,AlexandreAbraham,2015-06-12 07:53:05,"See #605. If the mirroring system proposed in this PR is validated, I will be able to move on this issue.
",,,see 605 If mirror system propos PR valid I abl move issu
issue_comment,481,nilearn,nilearn,AlexandreAbraham,2015-06-12 08:55:07,"See also #608. Used with the NiftiSpheresMasker, you get an atlas at no cost.
",,,see also 608 use niftispheresmask get atla cost
issue_comment,481,nilearn,nilearn,banilo,2015-06-12 09:02:10,"+1 for increasing scalability of dataset fetchers
",,,1 increas scalabl dataset fetcher
issue_comment,481,nilearn,nilearn,AlexandreAbraham,2015-07-06 17:07:44,"#637 make ADHD downloadable subject by subject.
",,,637 make adhd download subject subject
issue_comment,481,nilearn,nilearn,AlexandreAbraham,2016-06-07 22:01:30,"We broke down the datasets to the smallest chunks possible. I close this issue. Please re-open one for a specific dataset when you encounter a similar problem.
",,,We broke dataset smallest chunk possibl I close issu pleas reopen one specif dataset encount similar problem
