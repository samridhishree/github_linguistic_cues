rectype,issueid,project_owner,project_name,actor,time,text,action,title,clean_text
rectype,issueid,project_owner,project_name,actor,time,text,action,title,text
issue_title,915,nilearn,nilearn,GaelVaroquaux,2015-12-16 11:58:32,"We should add a downloader for the COBRE dataset:
https://figshare.com/articles/COBRE_preprocessed_with_NIAK_0_12_4/1160600

Via @pbellec, who might be able to answer questions about this data.
",start issue,Downloader for COBRE,We add download cobr dataset via pbellec might abl answer question data
issue_closed,915,nilearn,nilearn,AlexandreAbraham,2016-02-17 14:37:07,,closed issue,Downloader for COBRE,
issue_comment,915,nilearn,nilearn,AlexandreAbraham,2015-12-18 10:22:42,"I think that there is no need to mirror it. However, somebody should dig into the mat files to see if there is any useful information in there.
",,,I think need mirror howev somebodi dig mat file see use inform
issue_comment,915,nilearn,nilearn,AlexandreAbraham,2016-01-23 20:02:41,"But the data ils already in nifti... There is no need for conversion.
",,,but data il alreadi nifti there need convers
issue_comment,915,nilearn,nilearn,AlexandreAbraham,2016-01-25 09:38:49,"@pbellec, On the website, I see Nifti and .mat files. Aren't the covariates stored in the .mat? In that case, there is no need for conversion, they are readable in Python.

If you have an hdf5 file, can you share it? I already have scripts to convert them to other formats. Maybe I can help.
",,,pbellec On websit I see nifti mat file arent covari store mat In case need convers readabl python If hdf5 file share I alreadi script convert format mayb I help
issue_comment,915,nilearn,nilearn,AlexandreAbraham,2016-01-25 13:17:43,"Oh, OK! Now I get it.
",,,Oh OK now I get
issue_comment,915,nilearn,nilearn,AlexandreAbraham,2016-02-11 12:19:36,"@GaelVaroquaux Should we make an option to download part of the dataset? This dataset will only be used for analysis so I think that people will tend to download the whole stuff and not single subjects.
",,,gaelvaroquaux should make option download part dataset thi dataset use analysi I think peopl tend download whole stuff singl subject
issue_comment,915,nilearn,nilearn,AlexandreAbraham,2016-02-17 14:37:07,"Fixed in #989.
",,,fix 989
issue_comment,915,nilearn,nilearn,KamalakerDadi,2015-12-17 13:15:46,"> This is the one: nilearn/nilearn_sandbox#1

Yup, the link which @GaelVaroquaux provided has direct downloading of data without in need of asking for password. So, may be adapt code to this weblink or may download data and group upload them in nitrc ? @AlexandreAbraham 
",,,yup link gaelvaroquaux provid direct download data without need ask password So may adapt code weblink may download data group upload nitrc alexandreabraham
issue_comment,915,nilearn,nilearn,pbellec,2015-12-18 15:04:01,"The mat files contain all the covariates that have been regressed out of the data (motion parameters, mean CSF signal, etc). It also contains a list of time frames that have been removed from the time series by censoring for high motion. This means there are missing time points, which is not a problem for correlation, partial correlation, etc but needs to be taken into account as soon as you model the dynamics. 
",,,the mat file contain covari regress data motion paramet mean csf signal etc It also contain list time frame remov time seri censor high motion thi mean miss time point problem correl partial correl etc need taken account soon model dynam
issue_comment,915,nilearn,nilearn,pbellec,2015-12-18 15:05:52,"There is also a csv file with the demographic and clinical variables, as well as a measure of frame displacement (à la power), which is the average FD for all time frames left after censoring. 
",,,there also csv file demograph clinic variabl well measur frame displac à la power averag FD time frame left censor
issue_comment,915,nilearn,nilearn,GaelVaroquaux,2015-12-18 19:36:40,"> It also contains a list of time frames that have been removed from the
> time series by censoring for high motion. This means there are missing
> time points

That's nasty! It means that the data cannot be time-filtered. We
absolutely need to warn people clearly about this in the description of
the data, as they will overlook this fact, and apply eg time filtering.
",,,that nasti It mean data cannot timefilt We absolut need warn peopl clearli descript data overlook fact appli eg time filter
issue_comment,915,nilearn,nilearn,pbellec,2015-12-19 01:03:39,"I see. These data originate for a publication and have already been filtered smoothed etc (check the README.md file for full details by the way). Thinking about it, for the purpose of integration in nilearn, it may be better to have unsmooth data with minimal denoising applied. I could of course package the motion parameters, FD measures, CSF WM signal etc for people to regress out (or censor) if they want, but not enforce these choices on the user. Do you think this would be more useful? If you guys think it's worth it, I'd just make a separate ""minimally preprocessed"" release. 
",,,I see these data origin public alreadi filter smooth etc check readmemd file full detail way think purpos integr nilearn may better unsmooth data minim denois appli I could cours packag motion paramet FD measur csf WM signal etc peopl regress censor want enforc choic user Do think would use If guy think worth Id make separ minim preprocess releas
issue_comment,915,nilearn,nilearn,GaelVaroquaux,2015-12-19 10:12:34,"> Thinking about it, for the purpose of integration in nilearn, it may be
> better to have unsmooth data with minimal denoising applied. I could of
> course package the motion parameters, FD measures, CSF WM signal etc
> for people to regress out (or censor) if they want, but not enforce
> these choices on the user.

That would be awesome. We could easily have a switch in the downloader
that would download one or the other. We could also demonstrate in an
example how the NiftiMasker can be used to retrieve the full preprocessed
from the minimally preprocessed.
",,,that would awesom We could easili switch download would download one We could also demonstr exampl niftimask use retriev full preprocess minim preprocess
issue_comment,915,nilearn,nilearn,banilo,2015-12-19 22:35:13,"> by censoring for high motion

Isn't this what they call ""scrubbing""?
",,,isnt call scrub
issue_comment,915,nilearn,nilearn,pbellec,2015-12-20 04:47:48,"@GaelVaroquaux Then I'll work on that asap. 
@banilo you're right, the approach applied here is the (Power et al. Neuroimage 2012) scrubbing method, based on their FD measure. Power et al. (Neuroimage 2014) have investigated the idea of replacing the ""bad"" time points by values extrapolated from the ""good"" time points in the regression, and further started using the term censoring. But in this release the bad time points have simply been eliminated. 
",,,gaelvaroquaux then ill work asap banilo your right approach appli power et al neuroimag 2012 scrub method base FD measur power et al neuroimag 2014 investig idea replac bad time point valu extrapol good time point regress start use term censor but releas bad time point simpli elimin
issue_comment,915,nilearn,nilearn,KamalakerDadi,2016-01-18 15:45:48,"@pbellec Is there any source where I could download all datasets ? At the moment, I can download only data together of size https://ndownloader.figshare.com/articles/1160600/versions/15 1GB. Am I missing something here ?
",,,pbellec Is sourc I could download dataset At moment I download data togeth size 1gb Am I miss someth
issue_comment,915,nilearn,nilearn,KamalakerDadi,2016-01-23 09:41:04,"Any opinion regarding the format of the data? hdf5 ? 
question received from @pbellec 
",,,ani opinion regard format data hdf5 question receiv pbellec
issue_comment,915,nilearn,nilearn,GaelVaroquaux,2016-01-23 11:06:24,"> Any opinion regarding the format of the data? hdf5 ? question received from @pbellec

hdf5 is not an option as it would be bringing in externel dependencies.

We should abide by http://bids.neuroimaging.io/ Which means that we
should probably be using tab separated files. We can compress them in a
gz file.
",,,hdf5 option would bring externel depend We abid which mean probabl use tab separ file We compress gz file
issue_comment,915,nilearn,nilearn,pbellec,2016-01-12 03:07:53,"Sorry for the delay. Working on it. 
",,,sorri delay work
issue_comment,915,nilearn,nilearn,pbellec,2016-01-23 20:32:07,"@AlexandreAbraham it's for the covariates that come with the images (motion parameters, average WM and CSF signals, global signal, etc). Currently they are stored in a hdf5 file. I am going to try to format them in tsv files, taking inspiration from bids as @GaelVaroquaux suggests, and I'll see how it goes.   
",,,alexandreabraham covari come imag motion paramet averag WM csf signal global signal etc current store hdf5 file I go tri format tsv file take inspir bid gaelvaroquaux suggest ill see goe
issue_comment,915,nilearn,nilearn,pbellec,2016-02-10 09:30:48,"OK so this turned out to be [much more work](https://github.com/SIMEXP/niak/issues/122) than I had expected, as it forced me to separate the generation of confounds from their actual regression in the niak preprocessing pipeline. But hopefully this will help make the outputs much more re-usable so it's all good. Here is my proposition. 

There will be a file `<subject>_<session>_<run>_n.nii.gz` per subject. The _n suffix is for ""spatially normalized"", a la SPM. Actual processing steps will include: 
1.  Slice timing correction.
2.  Motion estimation (within- and between-run for each label).
3.  Linear and non-linear spatial normalization of the anatomical image.
4.  Coregistration of the anatomical volume with the target volume of 
5.  Concatenation of the T2-to-T1 and T1-to-stereotaxic-nl
6.  Resampling of the functional data in the stereotaxic space.

No regression of confounds, no smoothing. For each dataset, there will be a file `<subject>_<session>_<run>_confounds.tsv.gz`. It is a compressed file of tabulation-separated-values. See an example [here](https://drive.google.com/file/d/0B3fQhLcXWtDRb084YmczQ1lrRDA/view?usp=sharing). Each column is a confound, and the first row is a label. 

The labels will be explained in a separate json file, called [niak_confounds.json](https://drive.google.com/file/d/0B3fQhLcXWtDRNXpfeThTN3Q2eEE/view?usp=sharing). Note that some labels may be used multiple times: there are several covariates for slow time drift for example. I didn't want to include the frequency in the labels, because I did not want the labels to change from one dataset to the other. 

Hopefully, with that type of preprocessed data it will be easy to try alternative regression paths / smoothing directly from nilearn. Please let me know what you think. 

@chrisfilo I have tried to follow the spirit of BIDS here. Please let me know if you think this could be further improved in that direction. 
",,,OK turn much work I expect forc separ gener confound actual regress niak preprocess pipelin but hope help make output much reusabl good here proposit there file per subject the n suffix spatial normal la spm actual process step includ 1 slice time correct 2 motion estim within betweenrun label 3 linear nonlinear spatial normal anatom imag 4 coregistr anatom volum target volum 5 concaten t2tot1 t1tostereotaxicnl 6 resampl function data stereotax space No regress confound smooth for dataset file It compress file tabulationseparatedvalu see exampl each column confound first row label the label explain separ json file call niakconfoundsjson note label may use multipl time sever covari slow time drift exampl I didnt want includ frequenc label I want label chang one dataset hope type preprocess data easi tri altern regress path smooth directli nilearn pleas let know think chrisfilo I tri follow spirit bid pleas let know think could improv direct
issue_comment,915,nilearn,nilearn,pbellec,2016-02-11 03:50:50,"Not sure how much of a problem this is. Definitely not suitable for a quick tutorial, but this resource opens up possibilities for real data exploration. Would seem reasonable to have an option to download only a portion of the data. Another work-around would be to get time series on, say, 1000 or 3000 brain parcels. @GaelVaroquaux did not seem too keen on this option last time we talked. One last option would be to stick with 4D datasets but go down to a 4 or even 5 mm isotropic resolution. 
",,,not sure much problem definit suitabl quick tutori resourc open possibl real data explor would seem reason option download portion data anoth workaround would get time seri say 1000 3000 brain parcel gaelvaroquaux seem keen option last time talk one last option would stick 4D dataset go 4 even 5 mm isotrop resolut
issue_comment,915,nilearn,nilearn,pbellec,2016-02-12 00:52:49,"Alright, I'll try downsampling, and upload on zenodo. I'll check how fast is the download. I assume you are fine with the new format. Chris has started a document to formalize a BIDS preproc organization, but this is going to take a while so I'll move on with the current format. Will keep you posted. 
",,,alright ill tri downsampl upload zenodo ill check fast download I assum fine new format chri start document formal bid preproc organ go take ill move current format will keep post
issue_comment,915,nilearn,nilearn,KamalakerDadi,2016-02-10 14:17:51,"So, an impediment here:

I am writing a fetcher for https://figshare.com/articles/COBRE_preprocessed_with_NIAK_0_12_4/1160600  named as ""fully preprocessed"" and download time seems to be more than 1 hour fetching data from link ""Download all"". 

So, question? Any suggestions how to overcome this ? may be if we want to restrict downloading limited subjects ? Ping @GaelVaroquaux @AlexandreAbraham 
",,,So impedi I write fetcher name fulli preprocess download time seem 1 hour fetch data link download So question ani suggest overcom may want restrict download limit subject ping gaelvaroquaux alexandreabraham
issue_comment,915,nilearn,nilearn,GaelVaroquaux,2016-02-11 12:24:30,"Unlike @pbellec, I disagree that an hour download is not a problem. It's
a huge one.

However, I think that we should move on. The figshare interface makes it
hard to write code that does partial downloads. Figshare is also clearly
bandwidth limited: the download speed is limited on their side. What I
have in mind is that, in a latter PR, we will create a mirror of the data
on Nitrc, which has a fast download and makes it easier to organize
partial downloads.
",,,unlik pbellec I disagre hour download problem it huge one howev I think move the figshar interfac make hard write code partial download figshar also clearli bandwidth limit download speed limit side what I mind latter PR creat mirror data nitrc fast download make easier organ partial download
issue_comment,915,nilearn,nilearn,banilo,2016-01-12 22:35:44,"I am also working with COBRE data...
",,,I also work cobr data
issue_comment,915,nilearn,nilearn,KamalakerDadi,2015-12-16 22:19:26,"I think there is a PR in nilearn sandbox to get started and can be ported here. I guess.
",,,I think PR nilearn sandbox get start port I guess
issue_comment,915,nilearn,nilearn,KamalakerDadi,2016-01-11 16:32:07,"@pbellec Could you let us know if you have released the package suitable for integration in nilearn ? 
",,,pbellec could let us know releas packag suitabl integr nilearn
issue_comment,915,nilearn,nilearn,KamalakerDadi,2016-01-23 11:32:22,"@GaelVaroquaux Thank you. I will post this opinion to brain hack group.
",,,gaelvaroquaux thank I post opinion brain hack group
issue_comment,915,nilearn,nilearn,KamalakerDadi,2016-01-25 09:45:27,"I think the format we are discussing about is for new release with minimal preprocessed version.
",,,I think format discuss new releas minim preprocess version
