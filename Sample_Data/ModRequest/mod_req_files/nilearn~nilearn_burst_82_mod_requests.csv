rectype,sha,ins_del_count,issueid,actor,date,text,similarity
pull_request_commit,e8e6969f6eda3c6e5d4bb298598c93fafffc13a6,30,1143,MartinPerez,2016-07-01 22:58:12,extra doc corrections,NA
issue_comment,NA,NA,1143,MartinPerez,2016-06-21 19:20:29,"@AlexandreAbraham this is what I told you about, sorry I took some time to make the PR.
",0.615722253686971
pull_request_commit_comment,NA,NA,1143,bthirion,2016-06-21 20:46:23,"Could you add a test for that ?
",0.4906871165780683
issue_comment,NA,NA,1143,KamalakerDadi,2016-07-01 19:55:27,"Can you also have a look at `nilearn/decoding/space_net.py` ?
",0.488888203790668
issue_comment,NA,NA,1143,KamalakerDadi,2016-07-01 18:30:28,"Small remark on error message. Otherwise, LGTM.
",0.4872595468620412
issue_comment,NA,NA,1143,lesteve,2016-06-23 06:00:16,"> memory failure in one build?

Ignore this one, it is failing intermittently and unfortunately @aabadie did not find a way to make it more reliable.
",0.476433577591853
pull_request_commit_comment,NA,NA,1143,lesteve,2016-06-22 08:27:34,"Best practice: always include the value provided by the user in the error message, something like:

``` py
raise TypeError(""High pass must be float or None but you provided high_pass='{0}'"".format(high_pass))
```
",0.42840672411847946
issue_comment,NA,NA,1143,MartinPerez,2016-06-22 19:33:00,"memory failure in one build? could this be due to adding assert_raises errors with signal.clean? I dont understand why this would be the case.
",0.42574187787230844
pull_request_commit_comment,NA,NA,1143,KamalakerDadi,2016-07-01 18:28:08,"error message for wrong input has single quotes which I don't think it is necessary. May be we can remove single quotes ? What do you think ?

``` python
TypeError: low pass must be float or None but you provided low_pass='False'
```
",0.36387852196850823
 , , , , , , , 
 , , , , , , , 
pull_request_commit,11d804168a415206013404c551af94d18b29a4d0,30,1153,AlexandreAbraham,2016-07-06 12:07:22,Fix tests,NA
pull_request_commit_comment,NA,NA,1153,bthirion,2016-06-28 15:23:17,"On 28/06/2016 11:17, Gael Varoquaux wrote:

> In nilearn/connectome/connectivity_matrices.py 
> https://github.com/nilearn/nilearn/pull/1153#discussion_r68723710:
> 
> > ```
> >  """"""
> > ```
> > -    scaling = sqrt(2) \* np.ones(symmetric.shape[-2:])
> > -    np.fill_diagonal(scaling, 1.)
> > -    tril_mask = np.tril(np.ones(symmetric.shape[-2:])).astype(np.bool)
> > -    return symmetric[..., tril_mask] \* scaling[tril_mask]
> > -    if keep_diagonal:
> > -        scaling = sqrt(2) \* np.ones(symmetric.shape[-2:])
> > - scaling = sqrt(2) \* np.ones(symmetric.shape[-2:]) I proprose to 
> >   remove this scaling, which is horribly counter-intuitive. It might be 
> >   available under an 'isometric' option if there is any reason to keep 
> >   it, but I don't see any use case.
> >   I seem to remember that it is fairly important to have the metric to 
> >   really be the Cramer-Rao metric. That said, we could do it the other 
> >   way around, and apply 1/sqrt(2) on the diagonal, which would be 
> >   probably more intuitive.
> 
> â€”
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub 
> https://github.com/nilearn/nilearn/pull/1153/files/c79280449fff7c06e0fc5cdb645ad7f5d309ad92#r68723710, 
> or mute the thread 
> https://github.com/notifications/unsubscribe/AAOT1hZNtyTwjMeWvz1Sxo1DhMuyUE-Nks5qQOaTgaJpZM4I9wJA.
> 
> Sounds like a reasonable compromise.
",0.7586944043106961
pull_request_commit_comment,NA,NA,1153,bthirion,2016-06-27 21:36:07,"I proprose to remove this scaling, which is horribly counter-intuitive. 
It might be available under an 'isometric' option if there is any reason to keep it, but I don't  see any use case.
",0.4519615456573053
pull_request_commit_comment,NA,NA,1153,GaelVaroquaux,2016-06-28 09:17:07,"> > -        scaling = sqrt(2) \* np.ones(symmetric.shape[-2:])
> 
> I proprose to remove this scaling, which is horribly counter-intuitive.
> It might be available under an 'isometric' option if there is any reason to
> keep it, but I don't see any use case.

I seem to remember that it is fairly important to have the metric to
really be the Cramer-Rao metric.

That said, we could do it the other way around, and apply 1/sqrt(2) on
the diagonal, which would be probably more intuitive.
",0.4485099818280881
 , , , , , , , 
 , , , , , , , 
pull_request_commit,fd730e88953e8ea5e845d55fc39f4c9930143c3f,660,1148,ahoyosid,2016-07-07 22:58:34,removing some files after supporting >= 0.14,NA
issue_comment,NA,NA,1148,KamalakerDadi,2016-07-01 10:19:02,"> can someone among you look this PR up?

Sure, this is my top priority to look into. I might take some time to comment since I am new to look how backporting works in nilearn.

You may have to change title because your PR majorly addresses scikit-learn back ports.
",0.5083780310611772
issue_comment,NA,NA,1148,KamalakerDadi,2016-07-07 12:44:34,"Now that PR #1169 is merged. You can clean your PR a bit to remove dependencies related to < 0.14 (if I am not wrong).

Hopefully, this can also help us to solve coverage problem too.
",0.46758970169794667
 , , , , , , , 
 , , , , , , , 
