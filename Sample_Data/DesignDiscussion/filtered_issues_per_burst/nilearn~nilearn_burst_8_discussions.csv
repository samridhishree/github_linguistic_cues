Unnamed: 0,rectype,issueid,project_owner,project_name,actor,time,text,action,title
5,issue_comment,45,nilearn,nilearn,GaelVaroquaux,2013-04-18 08:19:16,"Imports should be from general to specific. Thus sklearn comes after scipy.

Naming conventions: n_confounds rather then confound_number

In docstrings, we usually say ""array-like"", rather than ""numpy.ndarray"", as other objects should work.

The return section is not correctly formatted. It won't build the docs right.

The remark on ordering faster is true only for numpy belove 1.7. Anyhow, I would try doing something like:

var = np.copy(series)
var **= 2
var = var.sum(axis=0)

I believe that this should always be fast (not tested, please correct me if I am wrong).

""Most energetic"" -> largest

thanks for the PR!
",nan,nan
8,issue_comment,45,nilearn,nilearn,pgervais,2013-04-18 09:09:33,"- The faster ordering remark is indeed true only for numpy 1.3, not for 1.7 (I just checked). This is not a problem for me, since nisl must work with numpy 1.3 (tell me if I'm wrong).
- The solution you give is slightly slower (of the order of 10%) than the best solution I found, though way better than the worst one. This is true for both numpy 1.3 and numpy 1.7. Ratio of execution time (best solution/your solution) are similar across versions. Here are the times I find on my mini-benchmark (the second times is your solution):

**C order** numpy 1.7: 1.0s vs 1.2s. numpy 1.3: 1.3s vs 1.6s.
**F order** numpy 1.7: 0.9s vs 1.1s. numpy 1.3: 0.9s vs 1.3s. 

Values were obtained on a single run, but are fairly reproducible. Uncertainty is around 0.05s. At least, differences are significant.
",nan,nan
6,issue_comment,45,nilearn,nilearn,GaelVaroquaux,2013-04-18 21:01:35,">   â€¢ The solution you give is slightly slower (of the order of 10%) than the
>     best solution I found, though way better than the worst one.

Then go for the solution I gave: in my experience this pattern is
applicable in many many spots. I would like to favor it in the code
(people learn to code by copy-pasting).
",nan,nan
9,issue_comment,45,nilearn,nilearn,pgervais,2013-04-19 07:35:31,"For @bthirion: I removed the prints. nosetests.tools.assert_true is in the present case almost equivalent to assert(). I made the change though.
The failure you experienced is probably due to a difference in computation error between my machine and yours, or to a different Scipy version. Try increasing the factor (set 20 instead of 15). If that doesn't work, tell me that I investigate further.

For @GaelVaroquaux: I used your solution in the code. I replaced var.sum() by var.mean() because there is no significant change in execution time, and if people are copy-pasting, they'll end up with a proper computation for variance, not for sum of squares.
",nan,nan
7,issue_comment,45,nilearn,nilearn,GaelVaroquaux,2013-04-19 12:54:14,"> nosetests.tools.assert_true is in the present case almost equivalent to
> assert().

It gives a better errror message.
",nan,nan
10,issue_comment,45,nilearn,nilearn,pgervais,2013-04-19 14:00:36,"The failure in test reported by @bthirion is real, but unrelated to this pull request. If nobody feels strongly against merging this, I'll do it on Monday.
",nan,nan
11,issue_comment,45,nilearn,nilearn,pgervais,2013-04-23 12:42:33,"@bthirion: I changed the factor in the test from 15 to 20 in commit 3fb93b2ed5b1b367c53, just pushed in branch master.
",nan,nan
 , , , , , , , , , 
 , , , , , , , , , 
