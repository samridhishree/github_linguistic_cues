Unnamed: 0,rectype,issueid,project_owner,project_name,actor,time,text,action,title
5,issue_comment,150,nilearn,nilearn,agramfort,2013-12-13 16:18:50,"travis failed due to old version of sklearn...
",nan,nan
4,issue_comment,150,nilearn,nilearn,mekman,2013-12-13 16:37:08,"Sorry about that! Could someone please give some advise on how to deal with that? The tests assuming a recent sklearn version where scoring can be specified as string. However the documentation states that nilearn depends on `Scikit-learn >= 0.12.1`. 

Do you prefer reverting back to the deprecated sklearn `score_func` or rely on a more recent sklearn version?
",nan,nan
6,issue_comment,150,nilearn,nilearn,agramfort,2013-12-13 20:18:57,"ping @GaelVaroquaux @AlexandreAbraham
",nan,nan
8,issue_comment,150,nilearn,nilearn,GaelVaroquaux,2013-12-13 20:34:37,"> ping @GaelVaroquaux @AlexandreAbraham

This feature will work only with a recent scikit-learn. Given that
nilearn is not even yet released, we don't want to start doing backports
and hacks.
",nan,nan
7,issue_comment,150,nilearn,nilearn,agramfort,2013-12-13 20:35:43,"how do you handle the travis issue?
",nan,nan
9,issue_comment,150,nilearn,nilearn,GaelVaroquaux,2013-12-13 20:48:44,"> how do you handle the travis issue?

Travis works fine in master. The pull request breaks Travis by adding an
option that is unsupported in older version of scikit-learn. In master,
this option is purposely ignored, and a warning is raised telling the
user to update scikit-learn.
",nan,nan
10,issue_comment,150,nilearn,nilearn,GaelVaroquaux,2013-12-15 15:34:38,"Closing this guy, as I don't think that we can merge it.
",nan,nan
 , , , , , , , , , 
 , , , , , , , , , 
4,issue_comment,144,nilearn,nilearn,fabianp,2013-12-03 08:30:34,"Would it be possible to reuse the maps in dataset_files.func instead of redoing the GLM? it seems to me that there are the response for the 8+1 categories
",nan,nan
5,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:06:41,"Can we rename the script to plot_....py, so that it is compiled during building of the webpage.
",nan,nan
33,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:10:14,"I think that we should maybe move this in another script. We are going to drown the beginning under too many things.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
34,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:10:56,"I would prefer if you used scipy.misc.imread, rather than the PIL
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
35,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:12:02,"We have decided to only use ""import matplotlib.pyplot as plt"" rather than ""pylab as pl""
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
36,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:13:57,"Ideally we want to use the NiftiMasker. We loose people when we go in these details.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
37,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:14:31,"Using strings are comments is going to confuse a lot beginners.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
38,pull_request_commit_comment,144,nilearn,nilearn,AlexandreAbraham,2013-12-03 09:18:13,"This is useful code anyway. The most important fact is that it points out that stimuli are not returned with absolute paths, which make them unusable for the end-user. I have fixed that and here is the same code to show them:

```
if take_a_look_at_stimuli:
    import Image
    import pylab as pl
    for stim_type, files in data_files.stimuli.items():
        if stim_type == ""controls"":
            # skip control images, there are too many
            continue
        pl.figure()
        for i in range(48):
            pl.subplot(6, 8, i + 1)
            try:
                pl.imshow(Image.open(files[i]), origin=""lower"")
                pl.gray()
                pl.axis(""off"")
            except:
                # just go to the next one if the file is not present
                continue
        pl.title(stim_type)
    pl.show()
```
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
39,pull_request_commit_comment,144,nilearn,nilearn,AlexandreAbraham,2013-12-03 09:25:27,"Awesome. `scipy.misc.imread` does not have the bug of random flipped images !
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
6,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:27:52,"Indeed, I agree with Fabian that the GLM is confusing.

It poses a bit of a problem, because the data in .func are not beta maps, but raw EPIs. In general, it is never a good idea to use the raw EPIs. With the Haxby dataset, we do it anyhow, but it's not great.

Ideally, we would have an object to estimate the betas from conditions. We don't have it currently. I don't know what is our best option here.
",nan,nan
40,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:43:17,"> This is useful code anyway.

It is. I just think that it should not be there.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
41,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-03 09:45:51,"Cool! Do I update this section with Alex's code and scipy.misc.imread or do I remove it altogether?
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
42,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-03 09:46:25,"OK, will correct and do this in the future
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
43,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:47:39,"> Cool! Do I update this section with Alex's code and scipy.misc.imread or do I
> remove it altogether?

Move it to a separate file and update it, please.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
44,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-03 09:51:35,"Two things:
1) When I tell NiftiMasker standardize=True, then the masked BOLD timecourses have neither zero mean nor unit variance. Not completely sure what is going on there. @AlexandreAbraham is looking into it I think. Otherwise I can do the same

2) Independently of this, the data seems to be naturally structured into 12 blocks of 121TR, which were probably acquired in different scanner runs. Detrending over the borders of these blocks makes no sense and neither does standardizing. If we want to use the NiftiMasker, we need to either break the file into these blocks and then mask or be able to pass breakpoints to the masker, such that it can pass them on using the bp kwarg to scipy.signal.detrend.

Seems overspecific but at the same time a rather recurrent phenomenon to me. Any thoughts?
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
45,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-03 09:52:08,"Just trying to distinguish citations from comments. Will remove this.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
7,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-03 09:55:57,"The structure of the GLM is taken from the paper and is rather specific. Looks like this is what worked best after trying a bunch of stuff. I don't think one can make an easily understandable object that generates this type of design matrix. But of course there is potential to simplify the code generating the design.
I deliberately used sklearn.linear_model.LinearRegression in order to avoid using linalg.pinv etc
",nan,nan
8,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:59:04,"Thinking more about the issue of the GLM: let's do as in the original article. I don't think that they used a GLM.
",nan,nan
9,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 10:00:19,"Ok, I need more time to understand the problem (right now I am multitasking too much).
",nan,nan
10,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 10:01:40,"Also, it would be good to have @bthirion's opinion on that.
",nan,nan
46,pull_request_commit_comment,144,nilearn,nilearn,AlexandreAbraham,2013-12-03 10:16:21,"I have tested and see no problem with the NiftiMasker.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
47,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-03 11:50:49,"Ok, do I gloss over boundary problems and just let the niftimasker do its job globally or do I take into account the structure of the data?
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
11,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-03 11:54:06,"They used a GLM and then computed similarities between conditions between even and odd runs.

I am not completely sure yet how they did their testing, since this leaves relatively few values for doing statistics

There is a very simple but pretty effective SVM decoding routine in the script I wrote. It currently reaches a .8 classification average over 9 classes. It is learned on single TRs. But it is just ""a stub"" in the sense that I did the absolutely simplest thing possible, not even removing rest data, but predicting it as a label.
",nan,nan
12,issue_comment,144,nilearn,nilearn,bthirion,2013-12-03 21:38:29,"As discussed today, the GLM is just meant to create averages something like
X_avg = np.array([X[y==i].mean(0) for in in np.unique(y)])
should suffice.
But, more importantly, classifier-based experiments are more compelling 
than cross-session correlations, so we may just drop this part of 
Haxby's historical experiments.
",nan,nan
13,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-04 14:00:36,"All GLMs gone, two SVM experiments coded.
To be optimized and possibly extended with logistic regression etc
",nan,nan
48,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 13:27:47,"don't you want kernel='linear' ?
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
49,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 13:28:32,"""ANOVA + Linear SVM C=1 on full brain""
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
50,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 13:29:27,"import matplotlib.pyplot as plt
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_stimuli.py')"
113,pull_request_commit_comment,144,nilearn,nilearn,AlexandreAbraham,2013-12-05 13:34:09,"Have you looked at the mask ? In my memories, `compute_epi_mask` has bad results on Haxby with default parameters.
",81d0aedfa135b33d8384eb74b7200634fc0c95a8,"(135, 103, u'plot_haxby_full_analysis.py')"
14,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-05 15:30:35,"yup, kernel=linear helps ... a lot ... especially in full brain
",nan,nan
15,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-05 15:31:45,"I did look at the cimputed epi mask, and to me it looked just fine. Then again, I only looked at some slices. Where do you think is the problem? Is it too small?
",nan,nan
51,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 15:34:06,"pep8 line too long
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
16,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-05 15:34:53,"Question: I know I am supposed to kick out the resting state intervals ... right now I am predicting them with the label ""rest"". If I do cut them out, I get the opportunity to do some hemodynamic shifting (I could do it without cutting, but still). Do I lag the data (TR=2.5s, 2TR=5s) or do I leave it as is?
",nan,nan
17,issue_comment,144,nilearn,nilearn,bthirion,2013-12-05 15:50:21,"Leave it as is.
",nan,nan
52,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-05 19:42:30,"Can you not do a function. I don't think that it is useful here, and it makes the example more complex.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_stimuli.py')"
53,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 19:47:57,"the function is used in the other script
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_stimuli.py')"
54,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 20:27:53,"matplotlib.pyplot
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
55,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 20:28:06,"remove these comments
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
56,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 20:33:50,"n_jobs=12 ???

use 1 by default please. For the poor people :)
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
57,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 20:34:30,"I would print the std rather than all the scores.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
58,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 20:36:32,"I get 0.69 mean score on full brain. Do you confirm?
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
59,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 20:37:51,"import matplotlib.pyplot as plt
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_stimuli.py')"
60,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-05 22:04:23,"I get .87 on full brain and .86 in the ROI.

this is without removing rest. Unfortunately I have uncommitted stuff on the other machine so I prefer continuing tomorrow
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
61,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-05 22:06:14,"I can remove the function call from the plot_haxby_full_analysis.py if it seems unncessary and change the stimulus visualization to script style. The reason for making a function was only this.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_stimuli.py')"
62,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-05 22:07:10,"0.69 is what I got without kernel=""linear"" on full brain btw
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
63,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-06 06:23:51,"> the function is used in the other script

But I would rather if this would not be the case. It forces to explain
way too many things when all we are trying to teach people is basic
nilearn usage.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_stimuli.py')"
18,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-06 16:44:58,"This is getting closer and closer to plot_haxby_decoding.py

Which direction do we take? Merge this with plot_haxby_decoding or make it into something different / more elaborate?
",nan,nan
19,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-06 16:58:50,"I had in mind that this example would be a bit more elaborate than the plot_haxby_decoding and stick more the to spirit of the original article: working on masks rather than doing a full brain analysis with anova, and reporting prediction power with bar plots for different categories of objects and different parts of the brain (using the masks provided with the dataset).

Does this sound reasonable?
",nan,nan
20,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-06 18:07:34,"Added code to look at other ROIs.

Not sure what to put in box plot since as of now scores are global classification accuracy.

Am I supposed to look at per class scores in different regions?
",nan,nan
21,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-06 18:17:23,"Well, I think that to answer thé neuroscientific question that was asked in thé original paper, scores on différent types of stimuli must be reported.
",nan,nan
22,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-06 18:51:57,"Definitely,

however I am afraid of the implications on code readability: I cannot create a scorer that returns multiple scores -- this will be blocked by cross_val_score as of one of the recent sklearn releases.

I can do everything by hand -- but in that case switching to one v one may be more understandable in terms of the code
",nan,nan
23,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-06 18:58:58,"forget about the last statement about one v one.

What I could do is write a scoring function that for each class returns true (and maybe false) positive scores.

And do a list comprehension iterating over a cv=KFold instead of a cross_val_score.

Does that sound like a compromise?
",nan,nan
24,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-06 21:31:23,"I would indeed do a manual for loop. The question is whether we simply
want to do the 'one vs all' manually, in a for loop, without even calling
it one vs all. That should probably be quite simple and not require
notions that beginners do not understand.
",nan,nan
64,pull_request_commit_comment,144,nilearn,nilearn,bthirion,2013-12-06 22:23:08,"cv=12 is supposed to amount to a leave-on-session-out cross-validation, I guess ? 
This is not obvious.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
65,pull_request_commit_comment,144,nilearn,nilearn,bthirion,2013-12-06 22:46:05,"Score: 0.67 +- 0.09 # on my laptop. I should run my experiments on your machine... what is you is number ?
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
66,pull_request_commit_comment,144,nilearn,nilearn,bthirion,2013-12-06 22:47:48,"Score: 0.60 +- 0.13 # here. After the tutorial, people will not only want to use nilearn, but use it on your box...
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
67,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-06 23:38:58,"is146139
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
68,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-06 23:39:49,"This is definitely weird...
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
69,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-06 23:41:40,"hmm, same on my laptop now. Will compare all software versions Monday
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
70,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-06 23:42:43,"got the same on my laptop.

I guess my computer is really powerful ...
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
71,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-07 18:34:17,"+1 for using LeaveOneLabelOut to make it explicit
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
72,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-08 01:17:43,"To make the example simpler, I would completely forgo the ANOVA + SVM part.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
73,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-08 01:19:34,"You need to give an rst title in the docstring, for rendering in the example gallery.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_stimuli.py')"
74,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-08 01:22:52,"Is there a reason that the vt mask is treated differently from the two others (eg face vs house)?

I think that the way that I would architecture that script is that I would do a loop over the masks names and a loop over the subjects inside that to produce the different values per subject and per mask, and end up with bar plots (or some other nice summary figure).
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
75,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-08 20:18:31,"No reason, except that I hadn't refactored yet, after adding the new functionality.

Restructuring done, but need a little more to make it nice.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
76,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-08 20:19:29,"I agree. 
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
77,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-08 20:22:02,"This may  be related to different scoring standards in possibly different versions of sklearn. 

Not sure though.

In any case, I will have to deal with a strong class imbalance problem while doing OvR for each label. Chance level for 0-1 scoring is at 87.5%
I am now going for f1_score. Any reason why I shouldn't?
If I go for AUC, I will have to dig into the classifier and move an offset around, which may not be ideal for a simple example
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
78,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-08 20:22:28,"yes
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
79,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-08 20:33:27,"F1_score is good. Thé standard accuracy score -by default- doesn't work if se do simple per-category scoring?eickenberg notifications@github.com a écrit :In plot_haxby_full_analysis.py:

> -    all_timecourses = brain_masker.fit_transform(data_files.func[subject_id])
> -    from sklearn.feature_selection import f_classif, SelectKBest
> -    from sklearn.pipeline import Pipeline
> -    feature_selection = SelectKBest(f_classif, 500)
> -    pipeline = Pipeline([(""Feature selection"", feature_selection),
> -                         (""Classifier"", classifier)])
>   +
> -    scores_anova_svm = cross_val_score(pipeline,
> -                                       all_timecourses[resting_state == False], 
> -                                       labels['labels'][resting_state == False],
> -                                       cv=12, n_jobs=1,
> -                                       verbose=True)
> -    # With resting removed state this scores at .91 (with RS: .87)
> -    print ""ANOVA + Linear SVM C=1 on full brain""
> -    print ""Score: %1.2f +- %1.2f"" % (scores_anova_svm.mean(),
> -                                     scores_anova_svm.std())
>   This may be related to different scoring standards in possibly different versions of sklearn. 

Not sure though.

In any case, I will have to deal with a strong class imbalance problem while doing OvR for each label. Chance level for 0-1 scoring is at 87.5%
I am now going for f1_score. Any reason why I shouldn't?
If I go for AUC, I will have to dig into the classifier and move an offset around, which may not be ideal for a simple example

—
Reply to this email directly or view it on GitHub.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
25,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-08 21:35:01,"The latest refactoring commit adds a rudimentary visualization of the f1_scores

https://github.com/eickenberg/nilearn/commit/c7c44e7010ee2e259d172ab7dcd91c3e99613f30

Performance is very bad except in mask_vt and mask_house. Very bad as in f1_score = 0.0

I have written (but not yet pushed) a cross-validation routine over the SVM C parameter. It looks like C > 1, e.g. C=1000 sometimes helps. The loop takes pretty long though. I was actually going to add a loop over classweights, since this had been discussed. But I think all this is very time consuming.

Any ideas on how to proceed? Let me know if I should push the crossvalidation routine
",nan,nan
26,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-08 21:46:17,"> Don't push the cross-validation routine.

To justify this: the goal is to have a very simple example, not the best
performing one. We want to be tutorial.
",nan,nan
27,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-08 21:46:26,"> Any ideas on how to proceed? Let me know if I should push the
> crossvalidation routine

Don't push the cross-validation routine. However, it is really good that
you have found that it helps a lot. I will use it in my tutorial to
stress the importance of cross-validation.
",nan,nan
28,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-08 22:33:46,"Ok. I was going to up the penalty to 100. (Not overfitting, of course -- I could have chosen this by chance, right? :))

But this makes the SVM have great difficulties treating the difficult stuff. I have the feeling it goes to max_iter before  converging. A waste of time that I won't push either.

The crossvalidation scheme will be on another branch once it is done.
",nan,nan
29,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-09 21:37:18,"As an alternative I just made a one versus one scheme which shows scores for all couplings of labels in a matrix. One for each region. So if necessary I can also push that up here.
",nan,nan
80,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-10 08:43:34,"same here you need title for rst rendering
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
81,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-10 08:45:15,"the figure layout is not perfect. Text goes out on the figure on my box.

Also I think the colorbar should reflect where the chance level is. It looks too black I feel.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
82,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-10 15:08:09,"Fixed the figure layout.

The scoring is done using f1_score. Where it is black I get no recall at all. So the score is bad.

If I use 0/1 scoring, the one vs rest chance level is at 87.5 % ....
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
83,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-10 15:08:31,"was did
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
84,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-11 00:46:31,"Thanks a lot. I'll be in the plane tomorrow, I think that I'll merge this
in, and modify a bit the example when I am in the plane, as I need to
converge on the course.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
30,issue_comment,144,nilearn,nilearn,fabianp,2013-12-11 08:53:17,"I've looked a bit into alternative classifiers. The mean of the scores_mean matrix is 0.26 using a LinearSVC as in the example and jumps to 0.30 if using a LogisticRegression with l1 penalty (default parameters). 

I plotted the difference is score in the confusion matrix (I can fix the colorbar and make it easier to read if you think it would be useful to show).

![difference](https://f.cloud.github.com/assets/277639/1722245/1e915b34-623d-11e3-9672-f4f80c3ef594.png)

so you win on almost all tasks (but not all!). Other stuff that I tried and did not get better results are Anova-SVM,  ensemble methods (RandomForestClassifier) and (surprisingly) a LogisticRegression with elasticnet penalty.

Please tell me if you would me to make a patch (in a single file or as an option for Michel's code?) for this or if you plan to say this in the oral/slides.
",nan,nan
31,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-11 19:27:57,"> Please tell me if you would me to make a patch (in a single file or as
> an option for Michel's code?)

That would be fantastic. As a single file, ie a new example. Thanks a
lot!
",nan,nan
32,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-11 23:11:57,"Merged. Thank you
",nan,nan
 , , , , , , , , , 
 , , , , , , , , , 
5,issue_comment,145,nilearn,nilearn,AlexandreAbraham,2013-12-05 13:05:28,"It happens to me that one or two voxels remains unselected so I'm +1 for this PR.
",nan,nan
6,issue_comment,145,nilearn,nilearn,AlexandreAbraham,2013-12-05 13:47:10,"As discussed, should we replace closing by a call to `fill_holes` ?
",nan,nan
7,issue_comment,145,nilearn,nilearn,GaelVaroquaux,2013-12-05 15:07:06,"> As discussed, should we replace closing by a call to fill_holes ?

No, I think that I prefer the closing. Indeed, some of the inner
ventricles may appear as holes in a brain mask. That's a good thing.

+1 for the closing. Thanks Ronald.
",nan,nan
8,issue_comment,145,nilearn,nilearn,GaelVaroquaux,2013-12-05 19:37:07,"From a very general standpoint, I am a bit worried by adding on more parameter, that non experts will not be able to set, especially since it is off by default.

This raises 2 questions:
- should it be on by default?
- can we merge it with the mask_opening, by saying that we do n openings, and (n-1) closings. Of course the name of the argument will have to be changed.
",nan,nan
9,issue_comment,145,nilearn,nilearn,rphlypo,2013-12-06 07:35:08,"I do agree with the additional parameter being a little cumbersome, and I do agree that we could set the number of openings and closings equal to each other, as such uniting these in a single operation.

The closings are on by default if a `NiftiMasker` object is initiated, but are off by default in the lower level function `compute_epi_mask` (this choice for backward compatibility). If we integrate both `opening` and `closing` into a single parameter/operation, I assume this question is automatically dealt with.

+1 for a simplified user interface with a single parameter, merging `opening` and `closing` into a single operation.
",nan,nan
10,issue_comment,145,nilearn,nilearn,GaelVaroquaux,2013-12-06 15:32:19,"> I do agree that we could set the number of openings and closings equal to each
> other, as such uniting these in a single operation.

OK, then we need to find a name for this parameter :)

> The closings are on by default if a NiftiMasker object is initiated, but are
> off by default in the lower level function compute_epi_mask (this choice for
> backward compatibility).

Let's not bother about backward compatibility for a package that isn't
even released.
",nan,nan
11,issue_comment,145,nilearn,nilearn,bthirion,2013-12-09 22:26:18,"The PR is technically correct, though I'm worried that we keep fixing this function with an ever-increasing list of parameters. Obviously, it is not possible to reduce it if the closing has to be smaller than the opening. 
Actually, I would have thought that people would prefer to rely on anatomically derived masks, I order to keep only gm. Are there so many datasets without anatomy provided with functional data ? Or any other pb behind ?
",nan,nan
12,issue_comment,145,nilearn,nilearn,GaelVaroquaux,2013-12-09 23:16:08,"We decided not to merge it with the current set of parameters, but to
contract the closing and opening in a single parameter.
",nan,nan
25,pull_request_commit_comment,145,nilearn,nilearn,rphlypo,2013-12-10 08:01:06,"Kept `opening` as the single parameter, since it is the opening that is of major importance to estimate the largest connected set from which the mask is derived.
",af535765ccd71496b5c6b2423a1320abfb7bab14,"(None, None, None)"
13,issue_comment,145,nilearn,nilearn,AlexandreAbraham,2014-01-30 15:18:43,"@rphlypo When addressing a comment, please reply so that people get notifications about it.
",nan,nan
18,pull_request_commit_comment,145,nilearn,nilearn,GaelVaroquaux,2014-01-30 15:57:00,"The docstring isn't completely clear to me. However, I have the impression that this is not what the code does: the code does first 2*n dilations, followed by n erosions.
",4d2dc1fc8428b6310ec2356bd85816c154608e43,"(None, '', u'nilearn/masking.py')"
14,issue_comment,145,nilearn,nilearn,GaelVaroquaux,2014-01-30 15:58:28,"Small remark on the docstring: @rphlypo could you please check that I am not mistaken and correct. Beyond that, :+1: for merge.
",nan,nan
15,issue_comment,145,nilearn,nilearn,rphlypo,2014-01-30 16:36:20,"docstring has been updated as per the request of @GaelVaroquaux 
",nan,nan
16,issue_comment,145,nilearn,nilearn,rphlypo,2014-01-30 16:37:23,"file has undergone an autoPEP8 for full PEP8 compliance
",nan,nan
17,issue_comment,145,nilearn,nilearn,GaelVaroquaux,2014-01-30 16:40:26,":+1: for merge. Thanks!
",nan,nan
 , , , , , , , , , 
 , , , , , , , , , 
